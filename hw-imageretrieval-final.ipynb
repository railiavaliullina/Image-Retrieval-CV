{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install easydict\n!pip install mlflow\n\nimport mlflow\n\nexp_name = \"without XBM\"\nmlflow.start_run(run_name=exp_name)","metadata":{"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting easydict\n  Downloading easydict-1.9.tar.gz (6.4 kB)\nBuilding wheels for collected packages: easydict\n  Building wheel for easydict (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for easydict: filename=easydict-1.9-py3-none-any.whl size=6350 sha256=7918bcfa25dd3e7b4bd752cb024b3fbfe6f67c383869fc42c78175d045d20d0d\n  Stored in directory: /root/.cache/pip/wheels/88/96/68/c2be18e7406804be2e593e1c37845f2dd20ac2ce1381ce40b0\nSuccessfully built easydict\nInstalling collected packages: easydict\nSuccessfully installed easydict-1.9\nCollecting mlflow\n  Downloading mlflow-1.15.0-py3-none-any.whl (14.2 MB)\n\u001b[K     |████████████████████████████████| 14.2 MB 6.8 MB/s eta 0:00:01     |████████████████████████▌       | 10.9 MB 6.3 MB/s eta 0:00:01     |████████████████████████████▎   | 12.5 MB 6.8 MB/s eta 0:00:01     |█████████████████████████████▌  | 13.0 MB 6.8 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: docker>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from mlflow) (4.4.1)\nRequirement already satisfied: sqlalchemy in /opt/conda/lib/python3.7/site-packages (from mlflow) (1.3.23)\nRequirement already satisfied: requests>=2.17.3 in /opt/conda/lib/python3.7/site-packages (from mlflow) (2.25.1)\nCollecting prometheus-flask-exporter\n  Downloading prometheus_flask_exporter-0.18.1.tar.gz (21 kB)\nRequirement already satisfied: Flask in /opt/conda/lib/python3.7/site-packages (from mlflow) (1.1.2)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.7/site-packages (from mlflow) (7.1.2)\nRequirement already satisfied: gitpython>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from mlflow) (3.1.13)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from mlflow) (1.1.5)\nRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.7/site-packages (from mlflow) (1.6.0)\nCollecting gunicorn\n  Downloading gunicorn-20.1.0.tar.gz (370 kB)\n\u001b[K     |████████████████████████████████| 370 kB 16.8 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from mlflow) (5.3.1)\nRequirement already satisfied: sqlparse>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from mlflow) (0.4.1)\nRequirement already satisfied: entrypoints in /opt/conda/lib/python3.7/site-packages (from mlflow) (0.3)\nCollecting querystring-parser\n  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nRequirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from mlflow) (2021.1)\nCollecting alembic<=1.4.1\n  Downloading alembic-1.4.1.tar.gz (1.1 MB)\n\u001b[K     |████████████████████████████████| 1.1 MB 16.3 MB/s eta 0:00:01\n\u001b[?25hCollecting databricks-cli>=0.8.7\n  Downloading databricks-cli-0.14.3.tar.gz (54 kB)\n\u001b[K     |████████████████████████████████| 54 kB 1.8 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from mlflow) (1.19.5)\nRequirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from mlflow) (3.15.6)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.7/site-packages (from alembic<=1.4.1->mlflow) (1.1.4)\nRequirement already satisfied: python-editor>=0.3 in /opt/conda/lib/python3.7/site-packages (from alembic<=1.4.1->mlflow) (1.0.4)\nRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from alembic<=1.4.1->mlflow) (2.8.1)\nRequirement already satisfied: tabulate>=0.7.7 in /opt/conda/lib/python3.7/site-packages (from databricks-cli>=0.8.7->mlflow) (0.8.9)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from databricks-cli>=0.8.7->mlflow) (1.15.0)\nRequirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from docker>=4.0.0->mlflow) (0.57.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from gitpython>=2.1.0->mlflow) (4.0.5)\nRequirement already satisfied: smmap<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow) (3.0.5)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.17.3->mlflow) (1.26.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.17.3->mlflow) (2020.12.5)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.17.3->mlflow) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.17.3->mlflow) (3.0.4)\nRequirement already satisfied: itsdangerous>=0.24 in /opt/conda/lib/python3.7/site-packages (from Flask->mlflow) (1.1.0)\nRequirement already satisfied: Jinja2>=2.10.1 in /opt/conda/lib/python3.7/site-packages (from Flask->mlflow) (2.11.3)\nRequirement already satisfied: Werkzeug>=0.15 in /opt/conda/lib/python3.7/site-packages (from Flask->mlflow) (1.0.1)\nRequirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from Jinja2>=2.10.1->Flask->mlflow) (1.1.1)\nRequirement already satisfied: setuptools>=3.0 in /opt/conda/lib/python3.7/site-packages (from gunicorn->mlflow) (49.6.0.post20210108)\nRequirement already satisfied: prometheus_client in /opt/conda/lib/python3.7/site-packages (from prometheus-flask-exporter->mlflow) (0.9.0)\nBuilding wheels for collected packages: alembic, databricks-cli, gunicorn, prometheus-flask-exporter\n  Building wheel for alembic (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158155 sha256=4a2ecbe41254dececde7d8e2aa539e3beb3d7e7b9912c1004a056af46c0c227d\n  Stored in directory: /root/.cache/pip/wheels/be/5d/0a/9e13f53f4f5dfb67cd8d245bb7cdffe12f135846f491a283e3\n  Building wheel for databricks-cli (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for databricks-cli: filename=databricks_cli-0.14.3-py3-none-any.whl size=100555 sha256=b3c40bf9ab11f2728b7eaa48cd574d911f26d457e334b15e9a2f960728df14d1\n  Stored in directory: /root/.cache/pip/wheels/3b/60/14/6930445b08959fbdf4e3029bac7e1f2cccb2e94df8afa00b29\n  Building wheel for gunicorn (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gunicorn: filename=gunicorn-20.1.0-py3-none-any.whl size=78917 sha256=cdb650976cffb4d34b665ccc35a43bd2ba7431cdebf1f7475865394d37e72623\n  Stored in directory: /root/.cache/pip/wheels/48/64/50/67e9a3524590218acb6a0c0f94038c0d60815866c52a667d57\n  Building wheel for prometheus-flask-exporter (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for prometheus-flask-exporter: filename=prometheus_flask_exporter-0.18.1-py3-none-any.whl size=17158 sha256=4a99c3c41c43aa1daf9bc4b2768b49a6b1296b438d937993406abfb38fc8c831\n  Stored in directory: /root/.cache/pip/wheels/c4/b6/b5/e76659f3b2a3a226565e27f0a7eb7a3ac93c3f4d68acfbe617\nSuccessfully built alembic databricks-cli gunicorn prometheus-flask-exporter\nInstalling collected packages: querystring-parser, prometheus-flask-exporter, gunicorn, databricks-cli, alembic, mlflow\n  Attempting uninstall: alembic\n    Found existing installation: alembic 1.5.8\n    Uninstalling alembic-1.5.8:\n      Successfully uninstalled alembic-1.5.8\nSuccessfully installed alembic-1.4.1 databricks-cli-0.14.3 gunicorn-20.1.0 mlflow-1.15.0 prometheus-flask-exporter-0.18.1 querystring-parser-1.2.4\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"<ActiveRun: >"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\" dataset config \"\"\"\n\nfrom easydict import EasyDict\n\ndataset_cfg = EasyDict()\n\ndataset_cfg.dataset_new_folder = '/kaggle/input/train-test-folders'  # 'E:/datasets/SOP_retrieval'\n# dataset_cfg.valid_query_retrieval_sets_path = 'E:/datasets/valid_dataset.pickle'\ndataset_cfg.test_query_retrieval_sets_path = '/kaggle/input/train-test-folders/test_dataset.pickle'  # 'E:/datasets/test_dataset.pickle'\n\ndataset_cfg.nb_categories = 12\ndataset_cfg.sz_dataset = 120053\ndataset_cfg.nb_elems_needed_for_product = 4\n\n# augmentation\ndataset_cfg.sz_crop = 224\ndataset_cfg.sz_resize = 256\ndataset_cfg.mean = [0.485, 0.456, 0.406]\ndataset_cfg.std = [0.229, 0.224, 0.225]\n\ndataset_cfg.load_image_folder = False #True\n\n\"\"\" eval config \"\"\"\n\nfrom easydict import EasyDict\n\neval_cfg = EasyDict()\n\neval_cfg.compute_metrics_before_training = False #True\neval_cfg.evaluate_on_train_data = False\neval_cfg.visualize_embeddings = False\n\n\"\"\" model config \"\"\"\n\nfrom easydict import EasyDict\n\nmodel_cfg = EasyDict()\n\nmodel_cfg.pretrained_model = True\nmodel_cfg.embedding_dim = 128\nmodel_cfg.random_seed = 0\n\n\"\"\" train config \"\"\"\n\nfrom easydict import EasyDict\n\ntrain_cfg = EasyDict()\n\ntrain_cfg.checkpoints_dir = '/kaggle/working/'  # 'D:/Users/Admin/PycharmProjects/imageretrievalvaliullina/'\ntrain_cfg.tensorboard_dir = '/kaggle/working/'\ntrain_cfg.device = 'cuda:0'\ntrain_cfg.nb_epochs = 100\ntrain_cfg.continue_training_from_epoch = False\ntrain_cfg.checkpoint_from_epoch = 0\ntrain_cfg.batch_size = 64\ntrain_cfg.lr = 1e-5\ntrain_cfg.weight_decay = 1e-4\ntrain_cfg.save_model = True\ntrain_cfg.log_to_mlflow = True\ntrain_cfg.use_gpu = True\ntrain_cfg.margin = 0.25\n\ntrain_cfg.train_on_kaggle = False\ntrain_cfg.path_to_image_folders = '/kaggle/input/train-test-folders'\n\ntrain_cfg.use_memory_bank = False\ntrain_cfg.memory_bank_iter = 0  # 1000\n\ntrain_cfg.overfit_on_batch = False\ntrain_cfg.overfit_on_batch_iters = 10000","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"\"\"\" Batch Sampler \"\"\"\n\nimport torch\nimport numpy as np\n\n\nclass BatchSampler(torch.utils.data.sampler.BatchSampler):\n    def __init__(self, dataset, batch_size, n=4, m=4, l=4):\n        self.n = n  # категорий\n        self.m = m  # продуктов\n        self.l = l  # изображений\n        self.dataset = dataset\n        self.batch_size = batch_size\n\n        self.category_labels = np.array(dataset.category_labels)\n        self.unique_category_labels = list(set(self.category_labels))\n\n        self.prod_labels = np.array(dataset.targets)\n        self.unique_prod_labels = list(set(self.prod_labels))\n\n        self.current_product_label_indices = self.get_all_labels_indices_with_current_label(\n            unique_labels=self.unique_prod_labels,\n            labels=self.prod_labels)\n\n    @staticmethod\n    def get_all_labels_indices_with_current_label(unique_labels, labels):\n        out = []\n        # для каждого уникального продукта/категории запоминаем все индексы\n        for c in unique_labels:\n            prod_label_indices = np.where(labels == c)[0]\n            np.random.shuffle(prod_label_indices)\n            out.append(prod_label_indices)\n        return out\n\n    def __len__(self):\n        return len(self.dataset) // self.batch_size\n\n    def __iter__(self):\n        for _ in range(len(self.dataset) // self.batch_size):\n            assert len(self.unique_category_labels) >= self.n, 'not enough categories'\n            chosen_categories = np.random.choice(len(self.unique_category_labels), self.n, replace=False)\n            chosen_categories_products = [self.dataset.categories_to_products[category] for category in chosen_categories]\n\n            chosen_products = []\n            for category_products in chosen_categories_products:\n                chosen_products.extend(np.random.choice(category_products, self.m, replace=False))\n\n            chosen_ids = []\n            for product in chosen_products:\n                cur_product_ids = self.current_product_label_indices[product]\n                assert len(cur_product_ids) >= self.l, f'not enough images, {cur_product_ids}, {chosen_categories}'\n                chosen_ids.extend(np.random.choice(cur_product_ids, self.l, replace=False))\n\n            assert len(chosen_ids) > 0\n            yield chosen_ids","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\"\"\" dataset \"\"\"\n\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport pickle\nfrom copy import deepcopy\nfrom collections import Counter\n\n# from data.BatchSampler import BatchSampler\n# from configs.dataset_config import cfg as dataset_cfg\n# from configs.train_config import cfg as train_config\n\n\ndef make_image_folder(dataset_type):\n    \"\"\"\n    Создание Image Folder с соответствующей аугментацией\n    \"\"\"\n    if dataset_type == 'train':\n        transforms_ = transforms.Compose([\n            transforms.RandomResizedCrop(dataset_cfg.sz_crop),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=dataset_cfg.mean, std=dataset_cfg.std)\n        ])\n    else:\n        transforms_ = transforms.Compose([\n            transforms.Resize(dataset_cfg.sz_resize),\n            transforms.CenterCrop(dataset_cfg.sz_crop),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=dataset_cfg.mean, std=dataset_cfg.std)\n        ])\n    image_folder = datasets.ImageFolder(root=f'/kaggle/input/train-test-folders/{dataset_type}/{dataset_type}', transform=transforms_)\n\n#     if dataset_type == 'train':\n#         # get category labels and dict {category_1: list of category_1 products, ...}\n#         categories_to_products = {k: [] for k in range(dataset_cfg.nb_categories)}\n#         image_folder.category_labels = []\n#         for im in image_folder.imgs:\n# #             print(im)\n#             category_label = int(im[0].split('/')[-3].split('_')[0])\n#             image_folder.category_labels.append(category_label)\n#             product_label = im[1]\n#             print(category_label, product_label)\n#             categories_to_products[category_label].append(product_label)\n#         image_folder.categories_to_products = {k: np.unique(v) for k, v in categories_to_products.items()}\n# #         image_folder.category_labels = [int(im[0].split('/')[-3].split('_')[0]) for im in image_folder.imgs]\n# #         image_folder.categories_to_products = categories_to_products\n\n    if dataset_type == 'train':\n        # get category labels and dict {category_1: list of category_1 products, ...}\n        categories_to_products = {k: [] for k in range(dataset_cfg.nb_categories)}\n        product_labels = [im[1] for im in image_folder.imgs]\n        product_labels_counter = Counter(product_labels)\n        product_labels_to_keep = [k for k, v in product_labels_counter.items() if v >= dataset_cfg.nb_elems_needed_for_product]\n        ids_to_keep = np.asarray([i for i, p in enumerate(product_labels) if p in product_labels_to_keep])\n        product_labels = list(np.asarray(product_labels)[ids_to_keep])\n        image_folder.imgs = list(np.asarray(image_folder.imgs)[ids_to_keep])\n\n        for i, im in enumerate(image_folder.imgs):\n            category_label = int(im[0].split('/')[-3].split('_')[0])\n            assert int(im[1]) == product_labels[i]\n            categories_to_products[category_label].append(product_labels[i])\n        categories_to_products = {k: np.unique(v) for k, v in categories_to_products.items()}\n        image_folder.category_labels = [int(im[0].split('/')[-3].split('_')[0]) for im in image_folder.imgs]\n        image_folder.categories_to_products = categories_to_products\n\n    # для обучения на Kaggle\n    with open(f'image_folder_{dataset_type}', 'wb') as f:\n        pickle.dump(image_folder, f)\n    return image_folder\n\n\ndef get_dataloader(dataset_type):\n    print(f'Getting {dataset_type} dataloader..')\n    with open(train_cfg.path_to_image_folders + f'/image_folder_{dataset_type}', 'rb') as f:\n        image_folder = pickle.load(f)\n    \n    if dataset_type == 'train':\n        batch_sampler = BatchSampler(image_folder, train_cfg.batch_size) if dataset_type == 'train' else None\n        dataloader = DataLoader(image_folder, batch_sampler=batch_sampler)\n        return dataloader\n\n    query_image_folder = deepcopy(image_folder)\n    retrieval_image_folder = deepcopy(image_folder)\n    imgs = get_query_and_retrieval_sets(image_folder, dataset_type, type='query')\n    query_image_folder.imgs = imgs\n    query_image_folder.samples = imgs\n    query_dataloader = DataLoader(query_image_folder, batch_size=train_cfg.batch_size)\n    print(f'got query dataloader, {len(query_dataloader)}')\n\n    imgs = get_query_and_retrieval_sets(image_folder, dataset_type, type='retrieval')\n    retrieval_image_folder.imgs = imgs\n    retrieval_image_folder.samples = imgs\n    retrieval_dataloader = DataLoader(retrieval_image_folder, batch_size=train_cfg.batch_size)\n    print(f'got retrieval dataloader, {len(retrieval_dataloader)}')\n    return (query_dataloader, retrieval_dataloader)","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\"\"\" triplet loss \"\"\"\n\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\n# from configs.train_config import cfg as train_cfg\n\n\nclass TripletLoss(nn.Module):\n    def __init__(self, margin):\n        super(TripletLoss, self).__init__()\n        self.margin = margin\n\n    @staticmethod\n    def get_distance(x1, x2):\n        mm = torch.mm(x1, x2.t())\n        dist = mm.diag().view((mm.diag().size()[0], 1))\n        dist = dist.expand_as(mm)\n        dist_ = dist + dist.t()\n        dist_ = (dist_ - 2 * mm).clamp(min=0)\n        return dist_.clamp(min=1e-4).sqrt()\n\n    def forward(self, inputs_col, targets_col, inputs_row, targets_row):\n        targets_col, targets_row = torch.tensor(targets_col), torch.tensor(targets_row)\n        b_size = inputs_col.size(0)\n        dists = self.get_distance(inputs_col, inputs_row)\n\n        p0 = targets_col.clone().view(1, targets_col.size()[0]).expand_as(dists)\n        p1 = targets_row.view(targets_row.size()[0], 1).expand_as(dists)\n\n        positives_ids = torch.eq(p0, p1).to(dtype=torch.uint8) - (torch.eye(len(dists)))#.to(self.device)\n        negatives_ids = (positives_ids == 0).to(dtype=torch.uint8) - (torch.eye(len(dists)))\n\n        losses_ = []\n        for i in range(b_size):\n            pos_ids_ = np.atleast_1d(positives_ids[i].nonzero().squeeze().cpu().numpy())\n            neg_ids_ = np.atleast_1d(negatives_ids[i].nonzero().squeeze().cpu().numpy())\n\n            pos_dists = dists[i, pos_ids_]\n            neg_dists = dists[i, neg_ids_]\n\n            pos_pair_expanded = pos_dists.expand(len(neg_ids_), len(pos_ids_)).T\n            neg_pair_expanded = neg_dists.expand(len(pos_ids_), len(neg_ids_))\n            all_possible_ids = (pos_pair_expanded + self.margin > neg_pair_expanded).to(dtype=torch.uint8).nonzero().squeeze().cpu().numpy()\n            if len(all_possible_ids) > 0:\n                pos_idxs, neg_idxs = (all_possible_ids[:, 0], all_possible_ids[:, 1]) if len(all_possible_ids.shape) > 1 \\\n                    else (all_possible_ids[0], all_possible_ids[1])\n\n                pos_dists_final = pos_dists[pos_idxs]\n                neg_dists_final = neg_dists[neg_idxs]\n\n                if isinstance(pos_idxs, np.int64) or isinstance(neg_idxs, np.int64):# or len(pos_idxs) < 1 or len(neg_idxs) < 1:\n                    pos_dists_final = pos_dists\n                    neg_ids = np.random.choice(len(neg_dists), len(pos_dists))\n                    neg_dists_final = neg_dists[neg_ids]\n            else:\n                pos_dists_final = pos_dists\n                neg_ids = np.random.choice(len(neg_dists), len(pos_dists))\n                neg_dists_final = neg_dists[neg_ids]\n\n            loss = torch.relu(pos_dists_final - neg_dists_final + self.margin)\n            if len(loss) > 0:\n                losses_.extend(loss)\n\n        loss = torch.stack(losses_).mean()\n        return loss","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\"\"\" memory bank \"\"\"\n\n\nimport torch\nimport numpy as np\n\n\nclass MemoryBank(object):\n    def __init__(self, embeddings_size, size):\n        self.embeddings = torch.tensor(np.zeros((size, embeddings_size))).float()#.cuda()\n        self.product_labels = torch.tensor(np.zeros(size)).float()#.cuda()\n        self.size = size\n        self.cur_size = 0\n\n    def update(self, embeddings, product_labels):\n        q_size = len(product_labels)\n        if self.cur_size + q_size > self.size:\n            self.embeddings[-q_size:] = embeddings\n            self.product_labels[-q_size:] = product_labels\n            self.cur_size = 0\n        else:\n            self.embeddings[self.cur_size: self.cur_size + q_size] = embeddings\n            self.product_labels[self.cur_size: self.cur_size + q_size] = product_labels\n            self.cur_size += q_size\n\n    def get_embeddings(self):\n        if self.product_labels[-1].item() != 0:\n            return self.embeddings, self.product_labels\n        else:\n            return self.embeddings[:self.cur_size], self.product_labels[:self.cur_size]","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\"\"\" resnet \"\"\"\n\n\nfrom torch.nn import AvgPool2d, Dropout, Linear\nimport torch.nn.functional as f\nimport numpy as np\nimport torchvision\nimport torch\n\n\ndef get_resnet50():\n    resnet50 = torchvision.models.resnet50(pretrained=True)\n    resnet50.features = torch.nn.Sequential(resnet50.conv1, resnet50.bn1, resnet50.relu, resnet50.maxpool,\n                                            resnet50.layer1,\n                                            resnet50.layer2, resnet50.layer3, resnet50.layer4)\n    resnet50.sz_features_output = 2048\n    for module in filter(lambda m: type(m) == torch.nn.BatchNorm2d, resnet50.modules()):\n        module.eval()\n        module.train = lambda _: None\n    return resnet50\n\n\ndef get_params_dict(model, emb_module_name):\n    dict_ = {k: [] for k in ['backbone', *emb_module_name]}\n    for name, param in model.named_parameters():\n        name = name.split('.')[0]\n        if name not in emb_module_name:\n            dict_['backbone'] += [param]\n        else:\n            dict_[name] += [param]\n    nb_total = len(list(model.parameters()))\n    nb_dict_params = sum([len(dict_[d]) for d in dict_])\n    assert nb_total == nb_dict_params\n    return dict_\n\n\ndef get_embedding(model, cfg):\n    model.features_pooling = AvgPool2d(7, stride=1, padding=0, ceil_mode=True, count_include_pad=True)\n    model.features_dropout = Dropout(0.01)\n    torch.random.manual_seed(cfg.random_seed)\n    np.random.seed(cfg.random_seed)\n\n    model.embedding = Linear(model.sz_features_output, cfg.embedding_dim).to(list(model.parameters())[0].device)\n    model.parameters_dict = get_params_dict(model=model, emb_module_name=['embedding'])\n\n    def forward(x):\n        x = model.features(x)\n        x = model.features_pooling(x)\n        bs = x.size(0)\n        x = x.view(bs, -1)\n        x = model.embedding(x)\n        x = f.normalize(x, p=2, dim=1)\n        return x\n\n    model.forward = forward\n\n\ndef get_model(cfg):\n    resnet50 = get_resnet50()\n    get_embedding(resnet50, cfg)\n    return resnet50\n","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\"\"\" data utils \"\"\"\n\n\nimport pickle\n\n# from configs.dataset_config import cfg as dataset_cfg\n\n\ndef get_query_and_retrieval_sets(image_folder, dataset_type, type):\n\n    query_retrieval_sets_path = dataset_cfg.valid_query_retrieval_sets_path if dataset_type == 'valid' \\\n        else dataset_cfg.test_query_retrieval_sets_path\n\n    with open(query_retrieval_sets_path, 'rb') as f:\n        query_retrieval_sets = pickle.load(f)\n\n    set_paths = query_retrieval_sets[type]\n    set_paths_and_labels = []\n\n    for i, im in enumerate(image_folder.imgs):\n        split = im[0].split(\"/\")\n        img_path = '/'.join([im_ for im_ in split[-3:]])\n        if img_path in set_paths:\n            set_paths_and_labels.append(('/'.join([im_ for im_ in split[:-3]]) + \"/\" + img_path, int(im[1])))\n\n    return set_paths_and_labels\n","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\"\"\" debug utils \"\"\"\n\n\nimport torch\nimport numpy as np\n\n# from configs.train_config import cfg as train_cfg\n# from utils.eval_utils import get_nearest_neighbors\n\n\ndef overfit_on_batch(dl, criterion, optimizer, model):\n    batch = next(dl)\n    images, labels = batch[0], batch[1]\n    for iter_ in range(train_cfg.overfit_on_batch_iters):\n        optimizer.zero_grad()\n        model, images = model.cuda(), images.cuda()\n        embeddings = model(images)\n        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n        loss = criterion(embeddings, labels, embeddings, labels)\n\n        assert not torch.isnan(loss).any(), 'loss is nan'\n        loss.backward()\n        optimizer.step()\n        top_1s = []\n        for i, emb in enumerate(embeddings):\n            neighbors = get_nearest_neighbors(torch.stack([em for em_i, em in enumerate(embeddings) if em_i != i]).detach().cpu().numpy().reshape(-1, 128),\n                                              [l.item() for j, l in enumerate(labels) if j != i],\n                                              emb.detach().cpu().numpy().reshape(-1, 128), k=1)\n            top_1 = 1 if labels[i].item() == neighbors else 0\n            top_1s.append(top_1)\n\n        print(f'iter: {iter_}, loss: {loss.item()}, top 1: {np.mean(top_1s)}')","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport pickle","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\"\"\" eval utils \"\"\"\n\nimport numpy as np\nimport torch\nfrom sklearn.neighbors import NearestNeighbors\n\n\ndef get_top_k(product_labels, neighbours_ids):\n    return np.mean(product_labels == neighbours_ids) * 100\n\n\ndef get_nearest_neighbors(retrieval_embeddings, query_embeddings, k=1):\n    index = NearestNeighbors(n_neighbors=k)\n    index.fit(retrieval_embeddings)\n    neighbors = index.kneighbors(query_embeddings)[1]\n    return neighbors\n\n\ndef evaluate(model, query_dl, retrieval_dl):\n    query_embeddings, query_labels = compute_embeddings(model, query_dl)\n    retrieval_embeddings, retrieval_labels = compute_embeddings(model, retrieval_dl)\n\n    neighbors = get_nearest_neighbors(retrieval_embeddings, query_embeddings)\n    n_neighbors = np.array([retrieval_labels[n[0]] for n in neighbors])\n    top_1 = get_top_k(query_labels, n_neighbors)\n    return top_1\n\n\ndef compute_embeddings(model, dl):\n    model = model.cuda()\n    print('Computing embeddings..')\n    dl_len = len(dl)\n    print(f'len: {dl_len}')\n    dl = iter(dl)\n    all_embeddings, all_labels = [], []\n    for i, batch in enumerate(dl):\n        if i % 50 == 0:\n            print(f'iter: {i}/{dl_len}')\n        x, y = batch[0], batch[1]\n        embeddings = model(x.cuda())\n#         embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n        all_labels.extend(y.data.cpu().numpy())\n        all_embeddings.extend(embeddings.data.cpu().numpy())\n    return all_embeddings, all_labels\n\n\ndef visualize_embeddings(embeddings_writer, model, dataloader, num_batches=5, tag=''):\n    all_embeddings, all_labels = [], []\n    dl = iter(dataloader)\n    for i in range(num_batches):\n        batch = next(dl)\n        images, labels = batch[0], batch[1]\n        embeddings = model(images)\n        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n        all_embeddings.extend(embeddings)\n        all_labels.extend(labels)\n\n    embeddings_writer.add_embedding(np.asarray([e.detach().numpy() for e in all_embeddings]),\n                                    metadata=np.asarray([a.item() for a in all_labels]), tag=tag)","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"\"\"\" train utils \"\"\"\n\nimport torch\n\n# from configs.train_config import cfg as train_cfg\n# from losses.triplet_loss import TripletLoss\n\n\ndef get_optimizer(model):\n    opt = torch.optim.Adam([\n        {'params': model.parameters(), 'lr': train_cfg.lr, 'weight_decay': train_cfg.weight_decay}])\n    return opt\n\n\ndef get_criterion():\n    criterion = TripletLoss(train_cfg.margin).cuda()\n    return criterion\n\n\ndef make_training_step(batch, criterion, optimizer, model, global_step, memory_bank):\n    optimizer.zero_grad()\n    images, labels = batch[0], batch[1]\n    model, images = model.cuda(), images.cuda()\n    embeddings = model(images)\n#     embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n    loss = criterion(embeddings, labels, embeddings, labels)\n\n    if train_cfg.use_memory_bank and global_step > train_cfg.memory_bank_iter:\n        print('sampling from bank')\n        mb_enbeddings, mb_features = memory_bank.get_embeddings()\n        mb_loss = criterion(embeddings, labels, mb_enbeddings, mb_features)\n        loss = loss + mb_loss\n\n    # loss = loss + mb_loss  # , embeddings, labels\n    assert not torch.isnan(loss).any(), 'loss is nan'\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n","metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"\"\"\" main \"\"\"\n\nimport torch\nimport time\nimport numpy as np\nfrom tensorboardX import SummaryWriter\nimport tarfile\nimport warnings\n\n# from data.dataset import get_dataloader\n# from models.resnet import get_model\n# from utils.train_utils import get_optimizer, get_criterion, make_training_step\n# from utils.eval_utils import visualize_embeddings\n# from utils.eval_utils import evaluate\n# from utils.debug_utils import overfit_on_batch\n# from configs.train_config import cfg as train_cfg\n# from configs.eval_config import cfg as eval_cfg\n# from configs.dataset_config import cfg as dataset_cfg\n# from configs.model_config import cfg as model_cfg\n# from models.memory_bank import MemoryBank\nwarnings.filterwarnings('ignore', category=UserWarning)\n\n\ndef train():\n    train_dataloader = get_dataloader(dataset_type='train')\n    query_test_dataloader, retrieval_test_dataloader = get_dataloader(dataset_type='test')\n\n    model = get_model(model_cfg)\n    optimizer = get_optimizer(model)\n    criterion = get_criterion()\n\n    if train_cfg.use_memory_bank:\n        memory_bank = MemoryBank(embeddings_size=128, size=len(train_dataloader.dataset))\n    else:\n        memory_bank = None\n\n    start_epoch, global_step = 0, -1\n\n    # loading saved checkpoints if needed\n    if train_cfg.continue_training_from_epoch:\n        try:\n            checkpoint = torch.load(train_cfg.checkpoints_dir + f'checkpoint_{train_cfg.checkpoint_from_epoch}.pth')\n            model.load_state_dict(checkpoint['model'])\n            start_epoch = checkpoint['epoch'] + 1\n            global_step = checkpoint['global_step'] + 1\n            optimizer.load_state_dict(checkpoint['opt'])\n        except FileNotFoundError:\n            print('Checkpoint not found')\n\n    # evaluate before training if needed\n    if eval_cfg.compute_metrics_before_training:\n        model.eval()\n\n        print(f'Evaluating on test data..')\n        top_1_test_accuracy = evaluate(model, query_test_dataloader, retrieval_test_dataloader)\n        print(f'Top-1 test accuracy: {top_1_test_accuracy}')\n\n        # visualize embeddings with tensorboard\n        if eval_cfg.visualize_embeddings:\n            embeddings_writer = SummaryWriter(log_dir=train_cfg.tensorboard_dir + f'/embeddings_vis/epoch_-1')\n            visualize_embeddings(embeddings_writer, model, train_dataloader, num_batches=5, tag='training_batch_before_training')\n        model.train()\n\n    if train_cfg.overfit_on_batch:\n        dl = iter(train_dataloader)\n        overfit_on_batch(dl, criterion, optimizer, model)\n\n    # main loop\n    for e in range(start_epoch, train_cfg.nb_epochs):\n        print(f'Epoch: {e}/{train_cfg.nb_epochs}')\n        epoch_start_time = time.time()\n        embeddings_writer = SummaryWriter(log_dir=train_cfg.tensorboard_dir + f'/embeddings_vis/epoch_{e}')\n\n        model.train()\n        print('Starting training..')\n        loss_list = []\n        len_ = len(train_dataloader)\n        \n        for i, batch in enumerate(train_dataloader):\n            loss = make_training_step(batch, criterion, optimizer, model, global_step, memory_bank)\n            loss_list.append(loss)\n            \n            mlflow.log_metric('loss', loss, global_step)\n            global_step += 1\n\n            if global_step % 50 == 0:\n                if global_step != 0:\n                    loss_mean = np.mean(loss_list[-50:])\n                else:\n                    loss_mean = loss\n                print(f'global step: {global_step}/{len_}, loss: {loss_mean}')\n\n        # save checkpoints\n        if train_cfg.save_model:\n            print('Saving current model...')\n            state = {\n                'model': model.state_dict(),\n                'epoch': e,\n                'global_step': global_step,\n                'opt': optimizer.state_dict()\n                }\n            torch.save(state, (train_cfg.checkpoints_dir + f'checkpoint_{e}.pth'))\n\n        # evaluate model\n        model.eval()\n        print(f'Evaluating on test data..')\n        top_1_test_accuracy = evaluate(model, query_test_dataloader, retrieval_test_dataloader)\n        print(f'Top-1 test accuracy: {top_1_test_accuracy}')\n        mlflow.log_metric('top-1 accuracy', top_1_test_accuracy, e)\n        model.train()\n\n        # visualize embeddings with tensorboard\n        if eval_cfg.visualize_embeddings:\n            visualize_embeddings(embeddings_writer, model, train_dataloader, num_batches=5,\n                                 tag='training_batch_during_training')\n\n        print(f'epoch training time: {round((time.time() - epoch_start_time) / 60, 3)} min')\n\n\nif __name__ == '__main__':\n    # if train_cfg.use_gpu:\n    #     torch.cuda.set_device(0)\n    np.random.seed(0)\n    torch.manual_seed(0)\n\n    total_training_start_time = time.time()\n    train()\n    print(f'Training time: {round((time.time() - total_training_start_time) / 60, 3)} min')","metadata":{"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Getting train dataloader..\nGetting test dataloader..\ngot query dataloader, 72\ngot retrieval dataloader, 305\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/97.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a409dc0a652478e9dc3ad0fe195198a"}},"metadata":{}},{"name":"stdout","text":"Epoch: 0/100\nStarting training..\nglobal step: 0/1124, loss: 0.15906189382076263\nglobal step: 50/1124, loss: 0.16462813913822175\nglobal step: 100/1124, loss: 0.16671728640794753\nglobal step: 150/1124, loss: 0.16550217539072037\nglobal step: 200/1124, loss: 0.16345396831631662\nglobal step: 250/1124, loss: 0.1599628384411335\nglobal step: 300/1124, loss: 0.16209630742669107\nglobal step: 350/1124, loss: 0.16195931315422057\nglobal step: 400/1124, loss: 0.16366865307092668\nglobal step: 450/1124, loss: 0.16001058861613274\nglobal step: 500/1124, loss: 0.1614264416694641\nglobal step: 550/1124, loss: 0.16144887536764144\n","output_type":"stream"}]},{"cell_type":"code","source":"# mlflow.log_metric('top-1 accuracy', 1, 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}